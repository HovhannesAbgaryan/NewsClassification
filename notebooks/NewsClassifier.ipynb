{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# News Classification with Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a29299caf3837ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naive Bayes classifier is well-known for its good performance on `text classification` task. Here we will go through the main concepts of text classification, before building our own model.\n",
    "\n",
    "## The Bag of Words Model\n",
    "\n",
    "One of the most important sub-tasks in pattern classification are `feature extraction` and `selection`. Prior to fitting the model and using machine learning algorithms for training, we need to think about how to best represent a text document as a `feature vector`. \n",
    "\n",
    "A commonly used model in Natural Language Processing (NLP) is the so-called `bag of words` model. The idea behind this model is very intuitive. First comes the creation of the `vocabulary` — the collection of all different words that occur in the training set and each word is associated with a count of how often it occurs.\n",
    "\n",
    "The vocabulary can then be used to construct the d-dimensional feature vectors for the individual documents where the dimensionality is equal to the number of different words in the vocabulary. This process is called `vectorization`.\n",
    "\n",
    "When doing the above feature extraction, we may come across whether we should consider `word occurrencies` (encoding with 0s (if word is not in text) and 1s (if word is in text)) or `word frequencies` (absolute counts of the words) in the text. The answer depends on the data, and it is necessary to try both approaches. In general, the first method is usually better when applied on small texts.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "`Tokenization` describes the general process of breaking down a text corpus into individual elements that serve as input for various NLP algorithms (we have performed this task earlier in the programming module). Usually, tokenization is accompanied by other optional processing steps, such as the removal of `stop words` and `punctuation characters`, `stemming` or `lemmatizing`, and the construction of `n-grams`. Below is an example of a simple but typical tokenization step that splits a sentence into individual words, removes punctuation, and converts all letters to lowercase.\n",
    "\n",
    "![tokenization](https://sebastianraschka.com/images/blog/2014/naive_bayes_1/tokenization-1.png)\n",
    "\n",
    "## Stop words\n",
    "\n",
    "Stop words are words that are particularly common in a text corpus and thus considered as rather un-informative (e.g., words such as `so`, `and`, `or`, `the`, ...). One approach to stop word removal is to search against a language-specific stop word dictionary. An alternative approach is to create a stop list by sorting all words in the entire text corpus by frequency. The stop list — after conversion into a set of non-redundant words — is then used to remove all those words from the input documents that are ranked among the top n words in this stop list.\n",
    "\n",
    "![stop words](https://sebastianraschka.com/images/blog/2014/naive_bayes_1/stop-1.png)\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "`Stemming` describes the process of transforming a word into its root form. The original stemming algorithm was developed my Martin F. Porter in 1979 and is hence known as Porter stemmer.\n",
    "\n",
    "![stemming](https://sebastianraschka.com/images/blog/2014/naive_bayes_1/porter-1.png)\n",
    "\n",
    "Stemming can create non-real words, such as \"thu\" in the example above. \n",
    "\n",
    "In contrast to stemming, `lemmatization` aims to obtain the canonical (grammatically correct) forms of the words, the so-called lemmas. Lemmatization is computationally more difficult and expensive than stemming, and in practice, both stemming and lemmatization have little impact on the performance of text classification.\n",
    "\n",
    "![lemmatization](https://sebastianraschka.com/images/blog/2014/naive_bayes_1/lemma-1.png)\n",
    "\n",
    "## N-Grams\n",
    "\n",
    "In the `n-gram` model, a token can be defined as a sequence of n items. The simplest case is the so-called unigram (1-gram) where each word consists of exactly one word, letter, or symbol. All previous examples were unigrams so far. Choosing the optimal number n depends on the language as well as the particular application. \n",
    "\n",
    "![n-grams](https://sebastianraschka.com/images/blog/2014/naive_bayes_1/grams-1.png)\n",
    "\n",
    "## Term Frequency - Inverse Document Frequency (Tf-idf)\n",
    "\n",
    "The term frequency - inverse document frequency (Tf-idf) is another alternative for characterizing text documents. It can be understood as a weighted term frequency, which is especially useful if stop words have not been removed from the text corpus. \n",
    "\n",
    "The Tf-idf approach assumes that the importance of a word is inversely proportional to how often it occurs across all documents. \n",
    "\n",
    "$$\\text{Tf-idf}=tf(t,d)\\cdot idf(t),$$\n",
    "\n",
    "where $tf(t,d)$ is the count of term $t$ in document $d$  \n",
    "$$idf(t)=log \\Big(\\frac{N+1\\cdot\\alpha}{df(t) + 1\\cdot\\alpha} \\Big) + 1,$$\n",
    "where $N$ is the number of documents in the corpus, $df(t)$ is the number of documents containing the term $t$ and $\\alpha = \\{0, 1\\}$ is the smoothing parameter."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2145a26e6441e19b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic examples of Vectorizing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a7c11fd30d7358c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:26:49.602178Z",
     "start_time": "2023-10-05T13:26:49.510813900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m tested \u001B[38;5;241m=\u001B[39m cv\u001B[38;5;241m.\u001B[39mtransform(X_test)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVocabulary\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_feature_names\u001B[49m()) \u001B[38;5;66;03m#vocabulary\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrain\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(trained\u001B[38;5;241m.\u001B[39mtoarray())\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "X_train = ['This is a sentence!', 'This is the other']\n",
    "X_test = [\"Is this a sentence?\", \"I am the second sentence\", \"And the third one!\", \"The sentence and a sentence\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Vectorizing with frequencies\n",
    "# cv = CountVectorizer() \n",
    "# cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Vectorizing with occurrences \n",
    "cv = CountVectorizer(binary = True)\n",
    "\n",
    "trained = cv.fit_transform(X_train)\n",
    "tested = cv.transform(X_test)\n",
    "\n",
    "print('Vocabulary')\n",
    "print(cv.get_feature_names()) #vocabulary\n",
    "print('Train')\n",
    "print(trained.toarray())\n",
    "print('Test')\n",
    "print(tested.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m tested1 \u001B[38;5;241m=\u001B[39m tfidf\u001B[38;5;241m.\u001B[39mtransform(X_test)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVocabulary\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtfidf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_feature_names\u001B[49m())\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIdf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(tfidf\u001B[38;5;241m.\u001B[39midf_)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# tfidf = TfidfVectorizer()\n",
    "# tfidf = TfidfVectorizer(smooth_idf=False)\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    " \n",
    "trained1 = tfidf.fit_transform(X_train)\n",
    "tested1 = tfidf.transform(X_test)\n",
    "\n",
    "print('Vocabulary')\n",
    "print(tfidf.get_feature_names())\n",
    "print('Idf')\n",
    "print(tfidf.idf_)\n",
    "print('Train')\n",
    "print(trained1.toarray())\n",
    "print('Test')\n",
    "print(tested1.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T19:42:43.285380Z",
     "start_time": "2023-10-04T19:42:43.211268800Z"
    }
   },
   "id": "7e3c8965d63e406d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To prevent CountVectorizer from removing symbols or separate chars use token_pattern!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9db59a9c171a63db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examples on real dataset\n",
    "We will be working on a real dataset that consists of news from two categories `army` and `economy` scraped from [this website](https://armenpress.am/eng/)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d954b7f53f7a6d62"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:25:46.217914200Z",
     "start_time": "2023-10-05T13:25:43.491066700Z"
    }
   },
   "id": "e996f1c7dffddae8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       article_title  \\\n0  Chief of General Staff of Armenian Armed Force...   \n1  Russian mobile lab deployed in Armenian milita...   \n2  1 out of 58 new confirmed coronavirus cases in...   \n3  Many quarantined Armenia servicemen return to ...   \n4    Armenia soldier wounded by Azerbaijani shooting   \n\n                                   article_paragraph  \n0  YEREVAN, APRIL 22, ARMENPRESS. Chief of Genera...  \n1  YEREVAN, APRIL 8, ARMENPRESS. The mobile lab w...  \n2  YEREVAN, MARCH 30, ARMENPRESS. 1 out of the 58...  \n3  YEREVAN, MARCH 30, ARMENPRESS. “Dozens” of qua...  \n4  YEREVAN, MARCH 27, ARMENPRESS. Soldier of the ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_title</th>\n      <th>article_paragraph</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Chief of General Staff of Armenian Armed Force...</td>\n      <td>YEREVAN, APRIL 22, ARMENPRESS. Chief of Genera...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Russian mobile lab deployed in Armenian milita...</td>\n      <td>YEREVAN, APRIL 8, ARMENPRESS. The mobile lab w...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1 out of 58 new confirmed coronavirus cases in...</td>\n      <td>YEREVAN, MARCH 30, ARMENPRESS. 1 out of the 58...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Many quarantined Armenia servicemen return to ...</td>\n      <td>YEREVAN, MARCH 30, ARMENPRESS. “Dozens” of qua...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Armenia soldier wounded by Azerbaijani shooting</td>\n      <td>YEREVAN, MARCH 27, ARMENPRESS. Soldier of the ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news1 = pd.read_csv(\"data/raw/armenpress_army.csv\", encoding=\"utf8\")\n",
    "news2 = pd.read_csv(\"data/raw/armenpress_economy.csv\", encoding=\"utf8\")\n",
    "\n",
    "news1.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:25:48.916574800Z",
     "start_time": "2023-10-05T13:25:48.739091700Z"
    }
   },
   "id": "ec3976003ed09371"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# As each article paragraph starts with the location and date of an event and the name of the news agency => this data does not provide any information that can be used for news classification => they can be deleted from article paragraphs\n",
    "news1.article_paragraph = news1.article_paragraph.str.split('[0-9], ARMENPRESS.', expand=True)[1]\n",
    "news2.article_paragraph = news2.article_paragraph.str.split('[0-9], ARMENPRESS.', expand=True)[1]\n",
    "\n",
    "# Attach a column for a label to be predicted (the label is the news type: military or economy)\n",
    "news1['type'] = 'military'\n",
    "news2['type'] = 'economy'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:25:52.538770700Z",
     "start_time": "2023-10-05T13:25:52.493539600Z"
    }
   },
   "id": "760da0b0b5edddf5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "article_title        0\narticle_paragraph    1\ntype                 0\ndtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join 2 data frames in order to work with united dataset\n",
    "news_df = pd.concat([news1, news2], axis=0, ignore_index=True)\n",
    "\n",
    "# Check if there are null values in dataset\n",
    "news_df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:25:55.300973Z",
     "start_time": "2023-10-05T13:25:55.256994700Z"
    }
   },
   "id": "5fdd0c8c6e6a54a1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "article_title        0\narticle_paragraph    0\ntype                 0\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If null values are in dataset (e.g. there is an article paragraph without title), then drop that data\n",
    "news_df = news_df.dropna()\n",
    "\n",
    "# Check once again for null values to make sure they are no longer in the dataset\n",
    "news_df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:25:58.293288800Z",
     "start_time": "2023-10-05T13:25:58.274903800Z"
    }
   },
   "id": "c1351a1ea555d0ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fitting on word frequencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "877efed0eab17a2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "News classification based on article paragraph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88b2c7308107fbd6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 279 instances\n",
      "Test dataset: 93 instances\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(news_df['article_paragraph'], news_df['type'], random_state = 0)\n",
    "\n",
    "print(\"Training dataset: {instance_count} instances\".format(instance_count=X_train.shape[0]))\n",
    "print(\"Test dataset: {instance_count} instances\".format(instance_count=X_test.shape[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:26:06.219918100Z",
     "start_time": "2023-10-05T13:26:06.175492100Z"
    }
   },
   "id": "4090b9c007607f72"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "type\nmilitary    48\neconomy     45\nName: count, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get counts of instances with all labels (i.e. count of military news and count of economy news) in test dataset\n",
    "y_test.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:26:09.405950300Z",
     "start_time": "2023-10-05T13:26:09.337008300Z"
    }
   },
   "id": "c044de87a22330f6"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# CountVectorizer object ignores pre-defined stop words of English\n",
    "frequency_vector_paragraph = CountVectorizer(stop_words ='english')\n",
    "\n",
    "# Creates a vocabulary from training data by associating each word with its frequency\n",
    "training_data = frequency_vector_paragraph.fit_transform(X_train)\n",
    "\n",
    "# Creates a vocabulary from testing data by associating each word with its frequency and using training vocabulary\n",
    "testing_data = frequency_vector_paragraph.transform(X_test)\n",
    "\n",
    "# count_vector.get_feature_names()\n",
    "# training_data.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:40:55.399584Z",
     "start_time": "2023-10-05T13:40:55.337300200Z"
    }
   },
   "id": "d675dcea05a5f666"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 279]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnaive_bayes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultinomialNB\n\u001B[0;32m      3\u001B[0m naive_bayes \u001B[38;5;241m=\u001B[39m MultinomialNB(alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-10\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m \u001B[43mnaive_bayes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[0;32m   1150\u001B[0m ):\n\u001B[1;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\naive_bayes.py:745\u001B[0m, in \u001B[0;36m_BaseDiscreteNB.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    724\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    725\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    726\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001B[39;00m\n\u001B[0;32m    727\u001B[0m \n\u001B[0;32m    728\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    743\u001B[0m \u001B[38;5;124;03m        Returns the instance itself.\u001B[39;00m\n\u001B[0;32m    744\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 745\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    746\u001B[0m     _, n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    748\u001B[0m     labelbin \u001B[38;5;241m=\u001B[39m LabelBinarizer()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\naive_bayes.py:578\u001B[0m, in \u001B[0;36m_BaseDiscreteNB._check_X_y\u001B[1;34m(self, X, y, reset)\u001B[0m\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_X_y\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    577\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\base.py:621\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    619\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    620\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 621\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    622\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    624\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\utils\\validation.py:1165\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1147\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1148\u001B[0m     X,\n\u001B[0;32m   1149\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1160\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1161\u001B[0m )\n\u001B[0;32m   1163\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[1;32m-> 1165\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\utils\\validation.py:409\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    407\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    408\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 409\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    410\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    411\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    412\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [2, 279]"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB(alpha=1e-10)\n",
    "naive_bayes.fit(training_data, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:30:24.419660800Z",
     "start_time": "2023-10-05T13:30:24.308482600Z"
    }
   },
   "id": "b7a28f543c36a75"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MultinomialNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotFittedError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mnaive_bayes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtesting_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy score by Scikit-learn accuracy metric: \u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy_score(y_test, predictions))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\naive_bayes.py:100\u001B[0m, in \u001B[0;36m_BaseNB.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m     87\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;124;03m    Perform classification on an array of test vectors X.\u001B[39;00m\n\u001B[0;32m     89\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03m        Predicted target values for X.\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 100\u001B[0m     \u001B[43mcheck_is_fitted\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    101\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_X(X)\n\u001B[0;32m    102\u001B[0m     jll \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_joint_log_likelihood(X)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\utils\\validation.py:1462\u001B[0m, in \u001B[0;36mcheck_is_fitted\u001B[1;34m(estimator, attributes, msg, all_or_any)\u001B[0m\n\u001B[0;32m   1459\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m is not an estimator instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (estimator))\n\u001B[0;32m   1461\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001B[1;32m-> 1462\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NotFittedError(msg \u001B[38;5;241m%\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(estimator)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m})\n",
      "\u001B[1;31mNotFittedError\u001B[0m: This MultinomialNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "predictions = naive_bayes.predict(testing_data)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy score by Scikit-learn accuracy metric: \", accuracy_score(y_test, predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:28:43.717778500Z",
     "start_time": "2023-10-05T13:28:43.640502900Z"
    }
   },
   "id": "378fae354ace6e2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def accuracy(actual, predicted):\n",
    "  return sum(actual == predicted) / len(predicted)\n",
    "\n",
    "print(\"Accuracy by custom accuracy function: \", accuracy(y_test, predictions))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bf6d4dbd29215c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Indexes and true labels of misclassified instances\n",
    "y_test[predictions != y_test]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39f19a6d39ffae51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Indexes and paragraphs of misclassified articles\n",
    "with pd.option_context('max_colwidth', 100):\n",
    "    print(X_test[predictions != y_test])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6e94962de01ee7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "News classification based on article title"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "feac47c13114d50"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 279 instances\n",
      "Test dataset: 93 instances\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train, y_test = train_test_split(news_df['article_title'], news_df['type'], random_state = 0)\n",
    "\n",
    "print(\"Training dataset: {instance_count} instances\".format(instance_count=X_train1.shape[0]))\n",
    "print(\"Test dataset: {instance_count} instances\".format(instance_count=X_test1.shape[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:45:31.486844300Z",
     "start_time": "2023-10-05T13:45:31.402931800Z"
    }
   },
   "id": "b06db9aa5a077b8e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "frequency_vector_title = CountVectorizer(stop_words ='english')\n",
    "training_data1 = frequency_vector_title.fit_transform(X_train1)\n",
    "testing_data1 = frequency_vector_title.transform(X_test1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:45:33.720420800Z",
     "start_time": "2023-10-05T13:45:33.670708600Z"
    }
   },
   "id": "e515b86bc0e12078"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m naive_bayes1\u001B[38;5;241m.\u001B[39mfit(training_data1, y_train)\n\u001B[0;32m      3\u001B[0m predictions \u001B[38;5;241m=\u001B[39m naive_bayes1\u001B[38;5;241m.\u001B[39mpredict(testing_data1)\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy score: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43maccuracy_score\u001B[49m(y_test, predictions))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "naive_bayes1 = MultinomialNB(alpha=1e-10)\n",
    "naive_bayes1.fit(training_data1, y_train)\n",
    "predictions = naive_bayes1.predict(testing_data1)\n",
    "print(\"Accuracy score by Scikit-learn accuracy metric: \", accuracy_score(y_test, predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:45:37.176928Z",
     "start_time": "2023-10-05T13:45:37.095094800Z"
    }
   },
   "id": "1b42e517c9e64f50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test[predictions != y_test]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32235472e20de0f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.option_context('max_colwidth', 100):\n",
    "  print(X_test1[predictions != y_test])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c6a9fb33a7ad85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fitting on word occurrences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e166b33ceb9e4d30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "News classification based on article paragraph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f06dc1a726a89295"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If binary is true in CountVectorizer object, then it will only consider whether the word is present in text (will be encoded as 1) or not (will be encoded as 0)\n",
    "occurrence_vector_paragraph = CountVectorizer(stop_words ='english', binary=True)\n",
    "\n",
    "training_data2 = occurrence_vector_paragraph.fit_transform(X_train)\n",
    "testing_data2 = occurrence_vector_paragraph.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0b5bac4346b74e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bernoulli_naive_bayes = BernoulliNB()\n",
    "bernoulli_naive_bayes.fit(training_data2, y_train)\n",
    "predictions = bernoulli_naive_bayes.predict(testing_data2)\n",
    "print(\"Accuracy score by Scikit-learn accuracy metric: \", accuracy_score(y_test, predictions))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef480484becb30f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model performance decreased when the word occurrences were given as input to the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bcc422d78c96bd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "News classification based on article title"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ab9682b4446304e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "occurrence_vector_title = CountVectorizer(stop_words = 'english', binary=True)\n",
    "training_data3 = occurrence_vector_title.fit_transform(X_train1)\n",
    "testing_data3 = occurrence_vector_title.transform(X_test1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1a855868f65a666"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bernoulli_naive_bayes = BernoulliNB()\n",
    "bernoulli_naive_bayes.fit(training_data3, y_train)\n",
    "predictions = bernoulli_naive_bayes.predict(testing_data3)\n",
    "print(\"Accuracy score by Scikit-learn accuracy metric: \", accuracy_score(y_test, predictions))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7760adac70b1abf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For article titles the model performance is same when we consider the word occurrences instead of word frequencies."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e505120a09027d20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fitting on tf-idf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc9d4385f5c574a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(stop_words = 'english')\n",
    "training_data4 = tfidf_vector.fit_transform(X_train)\n",
    "testing_data4 = tfidf_vector.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b9c814392d8c975"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB(alpha=1e-10)\n",
    "naive_bayes.fit(training_data4, y_train)\n",
    "predictions = naive_bayes.predict(testing_data4)\n",
    "print(\"Accuracy score by Scikit-learn accuracy metric: \", accuracy_score(y_test, predictions))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a76f293aa45ddf9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fitting on 2-grams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fdeb6df08c22b19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Concept explanation on simple example."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5651afc6e57775a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.', 'This document is the second document.', \n",
    "          'And this is the third one.', 'Is this the first document?']\n",
    "\n",
    "# Consider phrases made up of 1, 2 and 3 words\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())\n",
    "print(X2.toarray())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "430499fd32db8d91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Consider phrases made up of 1, 2, 3 and 4 words\n",
    "frequency_vector = CountVectorizer(stop_words ='english', ngram_range=(1, 4))\n",
    "training_data = frequency_vector.fit_transform(X_train)\n",
    "testing_data = frequency_vector.transform(X_test)\n",
    "\n",
    "naive_bayes = MultinomialNB(alpha=1e-10)\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "predictions = naive_bayes.predict(testing_data)\n",
    "print(\"Accuracy score by Scikit-learn accuracy metric: \", accuracy_score(y_test, predictions))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec274730ee0fc556"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "12d06006a42f5871"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a589204117defc60"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
